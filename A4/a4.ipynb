{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from transformers import CLIPTokenizer, CLIPTextModel, T5Tokenizer, T5EncoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = np.load(\"/ssd_scratch/cvit/souvik/subj1.npy\", allow_pickle=True).item()\n",
    "sub2 = np.load(\"/ssd_scratch/cvit/souvik/subj2.npy\", allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['language', 'vision', 'dmn', 'task'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((627, 11437), (627, 33792), (627, 17190), (627, 35120))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub1['language'].shape, sub1['vision'].shape, sub1['dmn'].shape, sub1['task'].shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/ssd_scratch/cvit/souvik/stimuli.txt', 'r') as file:\n",
    "    sentences = [line.strip() for line in file if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "model_large = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "model_small = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_name)\n",
    "model_clip = CLIPTextModel.from_pretrained(model_name)\n",
    "model_name_t5 = \"t5-base\"  # You can use different sizes like t5-small or t5-large\n",
    "tokenizer_t5 = T5Tokenizer.from_pretrained(model_name_t5)\n",
    "model_t5 = T5EncoderModel.from_pretrained(model_name_t5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_large = model_large.encode(sentences)\n",
    "embeddings_small = model_small.encode(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m mean_pooled\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     26\u001b[0m     embeddings_t5\u001b[38;5;241m.\u001b[39mappend(embedding)\n\u001b[0;32m---> 28\u001b[0m embeddings_large\u001b[38;5;241m.\u001b[39mshape, embeddings_small\u001b[38;5;241m.\u001b[39mshape, \u001b[43membeddings_clip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m, embeddings_t5\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "embeddings_clip = []\n",
    "for sentence in sentences:\n",
    "    inputs = tokenizer(sentence, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model_clip(**inputs)\n",
    "    \n",
    "    # Get the [CLS] token embedding which represents the whole sentence\n",
    "    embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "    embeddings_clip.append(embedding)\n",
    "\n",
    "\n",
    "embeddings_t5 = []\n",
    "for sentence in sentences:\n",
    "    inputs = tokenizer_t5(sentence, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model_t5(**inputs)\n",
    "    \n",
    "    # Get sentence embedding by averaging token embeddings\n",
    "    # Alternatively, you could use the last token or other pooling strategies\n",
    "    mask = inputs.attention_mask.unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()\n",
    "    masked_embeddings = outputs.last_hidden_state * mask\n",
    "    summed = torch.sum(masked_embeddings, dim=1)\n",
    "    counts = torch.clamp(torch.sum(mask, dim=1), min=1e-9)\n",
    "    mean_pooled = summed / counts\n",
    "    embedding = mean_pooled.numpy()\n",
    "    embeddings_t5.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((627, 768), (627, 384), (627, 512), (627, 768))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_clip = np.array(embeddings_clip).squeeze(1)\n",
    "embeddings_t5 = np.array(embeddings_t5).squeeze(1)  \n",
    "embeddings_large.shape, embeddings_small.shape, embeddings_clip.shape, embeddings_t5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def compute_2v2_accuracy(Y_true, Y_pred):\n",
    "    \"\"\"\n",
    "    Compute 2v2 accuracy between true and predicted voxel activations.\n",
    "    \n",
    "    Args:\n",
    "        Y_true: True voxel activations with shape (N, V) where N is number of samples and V is number of voxels\n",
    "        Y_pred: Predicted voxel activations with shape (N, V)\n",
    "        \n",
    "    Returns:\n",
    "        2v2 accuracy score (float)\n",
    "    \"\"\"\n",
    "    N = Y_true.shape[0]\n",
    "    count_correct = 0\n",
    "    \n",
    "    # Compute all pairwise comparisons\n",
    "    total_pairs = 0\n",
    "    for i in range(N-1):\n",
    "        for j in range(i+1, N):\n",
    "            # Compute cosine distances\n",
    "            cos_dist_i_i_hat = cosine(Y_true[i], Y_pred[i])\n",
    "            cos_dist_j_j_hat = cosine(Y_true[j], Y_pred[j])\n",
    "            cos_dist_i_j_hat = cosine(Y_true[i], Y_pred[j])\n",
    "            cos_dist_j_i_hat = cosine(Y_true[j], Y_pred[i])\n",
    "            \n",
    "            # 2v2 condition: sum of correct pairings < sum of incorrect pairings\n",
    "            if (cos_dist_i_i_hat + cos_dist_j_j_hat) < (cos_dist_i_j_hat + cos_dist_j_i_hat):\n",
    "                count_correct += 1\n",
    "            \n",
    "            total_pairs += 1\n",
    "    \n",
    "    return count_correct / total_pairs if total_pairs > 0 else 0\n",
    "\n",
    "def compute_pearson_correlation(Y_true, Y_pred):\n",
    "    \"\"\"\n",
    "    Compute average Pearson correlation between true and predicted voxel activations.\n",
    "    \n",
    "    Args:\n",
    "        Y_true: True voxel activations with shape (N, V)\n",
    "        Y_pred: Predicted voxel activations with shape (N, V)\n",
    "        \n",
    "    Returns:\n",
    "        Average Pearson correlation across all samples\n",
    "    \"\"\"\n",
    "    N = Y_true.shape[0]\n",
    "    correlations = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        corr, _ = pearsonr(Y_true[i], Y_pred[i])\n",
    "        correlations.append(corr)\n",
    "    \n",
    "    return np.mean(correlations)\n",
    "\n",
    "def cross_validate_ridge_regression(embeddings, voxels, n_folds=5, alphas=None):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation for ridge regression from embeddings to voxels.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: Sentence embeddings with shape (627, 384)\n",
    "        voxels: Voxel activations with shape (627, 10791)\n",
    "        n_folds: Number of folds for cross-validation\n",
    "        alphas: List of alpha values to try for ridge regression\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with average metrics across folds for each alpha\n",
    "    \"\"\"\n",
    "    if alphas is None:\n",
    "        alphas = [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "    \n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    results = {alpha: {'2v2_acc': [], 'pearson_corr': []} for alpha in alphas}\n",
    "    \n",
    "    for train_idx, test_idx in kf.split(embeddings):\n",
    "        # Split data into train and test sets for this fold\n",
    "        X_train, X_test = embeddings[train_idx], embeddings[test_idx]\n",
    "        y_train, y_test = voxels[train_idx], voxels[test_idx]\n",
    "        \n",
    "        for alpha in alphas:\n",
    "            # Train Ridge regression model\n",
    "            model = Ridge(alpha=alpha)\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Compute metrics\n",
    "            acc_2v2 = compute_2v2_accuracy(y_test, y_pred)\n",
    "            pearson_corr = compute_pearson_correlation(y_test, y_pred)\n",
    "            \n",
    "            # Store results for this fold\n",
    "            results[alpha]['2v2_acc'].append(acc_2v2)\n",
    "            results[alpha]['pearson_corr'].append(pearson_corr)\n",
    "    \n",
    "    # Compute average metrics across folds\n",
    "    avg_results = {}\n",
    "    for alpha in alphas:\n",
    "        avg_results[alpha] = {\n",
    "            '2v2_acc': np.mean(results[alpha]['2v2_acc']),\n",
    "            'pearson_corr': np.mean(results[alpha]['pearson_corr'])\n",
    "        }\n",
    "    \n",
    "    return avg_results, results, model\n",
    "\n",
    "def find_best_alpha(avg_results):\n",
    "    \"\"\"Find the alpha with the best performance based on both metrics\"\"\"\n",
    "    # Normalize both metrics to [0, 1] for fair comparison\n",
    "    acc_values = np.array([avg_results[alpha]['2v2_acc'] for alpha in avg_results])\n",
    "    corr_values = np.array([avg_results[alpha]['pearson_corr'] for alpha in avg_results])\n",
    "    \n",
    "    # Normalize (min-max scaling)\n",
    "    acc_norm = (acc_values - np.min(acc_values)) / (np.max(acc_values) - np.min(acc_values))\n",
    "    corr_norm = (corr_values - np.min(corr_values)) / (np.max(corr_values) - np.min(corr_values))\n",
    "    \n",
    "    # Compute combined score (equal weighting)\n",
    "    combined_scores = 0.5 * acc_norm + 0.5 * corr_norm\n",
    "    \n",
    "    # Get best alpha\n",
    "    alphas = list(avg_results.keys())\n",
    "    best_idx = np.argmax(combined_scores)\n",
    "    best_alpha = alphas[best_idx]\n",
    "    \n",
    "    return best_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validation results:\n",
      "Alpha\t2V2 Accuracy\tPearson Correlation\n",
      "--------------------------------------------------\n",
      "0.01\t0.0226\t\t0.7535\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.01]\n",
    "    \n",
    "# Perform cross-validation\n",
    "avg_results, detailed_results, model_ridge = cross_validate_ridge_regression(\n",
    "    embeddings_clip, sub1['language'], n_folds=5, alphas=alphas\n",
    ")\n",
    "\n",
    "# Print results for each alpha\n",
    "print(\"\\nCross-validation results:\")\n",
    "print(\"Alpha\\t2V2 Accuracy\\tPearson Correlation\")\n",
    "print(\"-\" * 50)\n",
    "for alpha in alphas:\n",
    "    print(f\"{alpha}\\t{avg_results[alpha]['2v2_acc']:.4f}\\t\\t{avg_results[alpha]['pearson_corr']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validation results:\n",
      "Alpha\t2V2 Accuracy\tPearson Correlation\n",
      "--------------------------------------------------\n",
      "0.01\t0.0416\t\t0.7939\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.01]\n",
    "    \n",
    "# Perform cross-validation\n",
    "avg_results, detailed_results, model_ridge = cross_validate_ridge_regression(\n",
    "    embeddings_clip, sub1['vision'], n_folds=5, alphas=alphas\n",
    ")\n",
    "\n",
    "# Print results for each alpha\n",
    "print(\"\\nCross-validation results:\")\n",
    "print(\"Alpha\\t2V2 Accuracy\\tPearson Correlation\")\n",
    "print(\"-\" * 50)\n",
    "for alpha in alphas:\n",
    "    print(f\"{alpha}\\t{avg_results[alpha]['2v2_acc']:.4f}\\t\\t{avg_results[alpha]['pearson_corr']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validation results:\n",
      "Alpha\t2V2 Accuracy\tPearson Correlation\n",
      "--------------------------------------------------\n",
      "0.01\t0.7767\t\t0.6094\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.01]\n",
    "    \n",
    "# Perform cross-validation\n",
    "avg_results, detailed_results, model_ridge_lang = cross_validate_ridge_regression(\n",
    "    embeddings_small, sub1['language'], n_folds=5, alphas=alphas\n",
    ")\n",
    "\n",
    "# Print results for each alpha\n",
    "print(\"\\nCross-validation results:\")\n",
    "print(\"Alpha\\t2V2 Accuracy\\tPearson Correlation\")\n",
    "print(\"-\" * 50)\n",
    "for alpha in alphas:\n",
    "    print(f\"{alpha}\\t{avg_results[alpha]['2v2_acc']:.4f}\\t\\t{avg_results[alpha]['pearson_corr']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validation results:\n",
      "Alpha\t2V2 Accuracy\tPearson Correlation\n",
      "--------------------------------------------------\n",
      "0.01\t0.7831\t\t0.6751\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.01]\n",
    "    \n",
    "# Perform cross-validation\n",
    "avg_results, detailed_results, model_ridge_vision = cross_validate_ridge_regression(\n",
    "    embeddings_small, sub1['vision'], n_folds=5, alphas=alphas\n",
    ")\n",
    "\n",
    "# Print results for each alpha\n",
    "print(\"\\nCross-validation results:\")\n",
    "print(\"Alpha\\t2V2 Accuracy\\tPearson Correlation\")\n",
    "print(\"-\" * 50)\n",
    "for alpha in alphas:\n",
    "    print(f\"{alpha}\\t{avg_results[alpha]['2v2_acc']:.4f}\\t\\t{avg_results[alpha]['pearson_corr']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validation results:\n",
      "Alpha\t2V2 Accuracy\tPearson Correlation\n",
      "--------------------------------------------------\n",
      "0.01\t0.7120\t\t0.4054\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.01]\n",
    "    \n",
    "# Perform cross-validation\n",
    "avg_results, detailed_results, model_ridge_task = cross_validate_ridge_regression(\n",
    "    embeddings_small, sub1['task'], n_folds=5, alphas=alphas\n",
    ")\n",
    "\n",
    "# Print results for each alpha\n",
    "print(\"\\nCross-validation results:\")\n",
    "print(\"Alpha\\t2V2 Accuracy\\tPearson Correlation\")\n",
    "print(\"-\" * 50)\n",
    "for alpha in alphas:\n",
    "    print(f\"{alpha}\\t{avg_results[alpha]['2v2_acc']:.4f}\\t\\t{avg_results[alpha]['pearson_corr']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validation results:\n",
      "Alpha\t2V2 Accuracy\tPearson Correlation\n",
      "--------------------------------------------------\n",
      "0.01\t0.6872\t\t0.3926\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.01]\n",
    "    \n",
    "# Perform cross-validation\n",
    "avg_results, detailed_results, model_ridge_dmn = cross_validate_ridge_regression(\n",
    "    embeddings_small, sub1['dmn'], n_folds=5, alphas=alphas\n",
    ")\n",
    "\n",
    "# Print results for each alpha\n",
    "print(\"\\nCross-validation results:\")\n",
    "print(\"Alpha\\t2V2 Accuracy\\tPearson Correlation\")\n",
    "print(\"-\" * 50)\n",
    "for alpha in alphas:\n",
    "    print(f\"{alpha}\\t{avg_results[alpha]['2v2_acc']:.4f}\\t\\t{avg_results[alpha]['pearson_corr']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def mask_by_pos(text, pos_to_keep):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.text if token.pos_ in pos_to_keep else '' for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hesitantly skied down the steep trail that my buddies convinced me to try. |       trail   buddies      |   skied        convinced   try  |      steep         \n"
     ]
    }
   ],
   "source": [
    "sentence = sentences[-10]\n",
    "masked_nouns = mask_by_pos(sentence, {\"NOUN\"})\n",
    "masked_verbs = mask_by_pos(sentence, {\"VERB\"})\n",
    "masked_adjs = mask_by_pos(sentence, {\"ADJ\"})\n",
    "print(sentence, \"|\",masked_nouns, \"|\",masked_verbs, \"|\",masked_adjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_embedding(text):\n",
    "    emb_big = model_large.encode([text])\n",
    "    emb_small = model_small.encode([text])\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model_clip(**inputs)\n",
    "    emb_clip = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "    return emb_big, emb_small, emb_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384,) (1, 384) (1, 384) (1, 384)\n"
     ]
    }
   ],
   "source": [
    "embedding_test_full = embeddings_small[-10]\n",
    "embedding_test_nouns = get_single_embedding(masked_nouns)[1]\n",
    "embedding_test_verbs = get_single_embedding(masked_verbs)[1]\n",
    "embedding_test_adjs = get_single_embedding(masked_adjs)[1]\n",
    "print(embedding_test_full.shape, embedding_test_nouns.shape, embedding_test_verbs.shape, embedding_test_adjs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11437,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmri_list[0][-10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation with language for nouns: 0.5355\n",
      "Correlation with vision for nouns: 0.5889\n",
      "Correlation with task for nouns: 0.4942\n",
      "Correlation with dmn for nouns: 0.4902\n"
     ]
    }
   ],
   "source": [
    "# lets test for nouns first\n",
    "model_lists = [model_ridge_lang, model_ridge_vision, model_ridge_task, model_ridge_dmn]\n",
    "model_names = [\"language\", \"vision\", \"task\", \"dmn\"]\n",
    "fmri_list = [sub1['language'], sub1['vision'], sub1['task'], sub1['dmn']]\n",
    "for i, model in enumerate(model_lists):\n",
    "    pred_fmri_noun = model.predict(embedding_test_nouns)\n",
    "    pred_fmri_noun = pred_fmri_noun.squeeze(0)\n",
    "    corr,_ = pearsonr(fmri_list[i][-10], pred_fmri_noun)\n",
    "    print(f\"Correlation with {model_names[i]} for nouns: {corr:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation with language for verbs: 0.5720\n",
      "Correlation with vision for verbs: 0.7075\n",
      "Correlation with task for verbs: 0.2898\n",
      "Correlation with dmn for verbs: 0.2352\n"
     ]
    }
   ],
   "source": [
    "# verbs\n",
    "model_lists = [model_ridge_lang, model_ridge_vision, model_ridge_task, model_ridge_dmn]\n",
    "model_names = [\"language\", \"vision\", \"task\", \"dmn\"]\n",
    "fmri_list = [sub1['language'], sub1['vision'], sub1['task'], sub1['dmn']]\n",
    "for i, model in enumerate(model_lists):\n",
    "    pred_fmri_verbs = model.predict(embedding_test_verbs)\n",
    "    pred_fmri_verbs = pred_fmri_verbs.squeeze(0)\n",
    "    corr,_ = pearsonr(fmri_list[i][-10], pred_fmri_verbs)\n",
    "    print(f\"Correlation with {model_names[i]} for verbs: {corr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation with language for verbs: 0.2614\n",
      "Correlation with vision for verbs: 0.5526\n",
      "Correlation with task for verbs: 0.1297\n",
      "Correlation with dmn for verbs: -0.0942\n"
     ]
    }
   ],
   "source": [
    "# adjs\n",
    "model_lists = [model_ridge_lang, model_ridge_vision, model_ridge_task, model_ridge_dmn]\n",
    "model_names = [\"language\", \"vision\", \"task\", \"dmn\"]\n",
    "fmri_list = [sub1['language'], sub1['vision'], sub1['task'], sub1['dmn']]\n",
    "for i, model in enumerate(model_lists):\n",
    "    pred_fmri_adjs = model.predict(embedding_test_adjs)\n",
    "    pred_fmri_adjs = pred_fmri_adjs.squeeze(0)\n",
    "    corr,_ = pearsonr(fmri_list[i][-10], pred_fmri_adjs)\n",
    "    print(f\"Correlation with {model_names[i]} for verbs: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validation results:\n",
      "Alpha\t2V2 Accuracy\tPearson Correlation\n",
      "--------------------------------------------------\n",
      "0.01\t0.9958\t\t0.3266\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.01]\n",
    "    \n",
    "# Perform cross-validation\n",
    "avg_results, detailed_results, model_ridge = cross_validate_ridge_regression(\n",
    "    sub1['language'],embeddings_small, n_folds=5, alphas=alphas\n",
    ")\n",
    "\n",
    "# Print results for each alpha\n",
    "print(\"\\nCross-validation results:\")\n",
    "print(\"Alpha\\t2V2 Accuracy\\tPearson Correlation\")\n",
    "print(\"-\" * 50)\n",
    "for alpha in alphas:\n",
    "    print(f\"{alpha}\\t{avg_results[alpha]['2v2_acc']:.4f}\\t\\t{avg_results[alpha]['pearson_corr']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validation results:\n",
      "Alpha\t2V2 Accuracy\tPearson Correlation\n",
      "--------------------------------------------------\n",
      "0.01\t0.9967\t\t0.3487\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.01]\n",
    "    \n",
    "# Perform cross-validation\n",
    "avg_results, detailed_results, model_ridge = cross_validate_ridge_regression(\n",
    "    sub1['language'],embeddings_large, n_folds=5, alphas=alphas\n",
    ")\n",
    "\n",
    "# Print results for each alpha\n",
    "print(\"\\nCross-validation results:\")\n",
    "print(\"Alpha\\t2V2 Accuracy\\tPearson Correlation\")\n",
    "print(\"-\" * 50)\n",
    "for alpha in alphas:\n",
    "    print(f\"{alpha}\\t{avg_results[alpha]['2v2_acc']:.4f}\\t\\t{avg_results[alpha]['pearson_corr']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validation results:\n",
      "Alpha\t2V2 Accuracy\tPearson Correlation\n",
      "--------------------------------------------------\n",
      "0.01\t0.0000\t\t1.0000\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.01]\n",
    "    \n",
    "# Perform cross-validation\n",
    "avg_results, detailed_results, model_ridge = cross_validate_ridge_regression(\n",
    "    sub1['language'],embeddings_clip, n_folds=5, alphas=alphas\n",
    ")\n",
    "\n",
    "# Print results for each alpha\n",
    "print(\"\\nCross-validation results:\")\n",
    "print(\"Alpha\\t2V2 Accuracy\\tPearson Correlation\")\n",
    "print(\"-\" * 50)\n",
    "for alpha in alphas:\n",
    "    print(f\"{alpha}\\t{avg_results[alpha]['2v2_acc']:.4f}\\t\\t{avg_results[alpha]['pearson_corr']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validation results:\n",
      "Alpha\t2V2 Accuracy\tPearson Correlation\n",
      "--------------------------------------------------\n",
      "0.01\t0.9958\t\t0.6499\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.01]\n",
    "    \n",
    "# Perform cross-validation\n",
    "avg_results, detailed_results, model_ridge = cross_validate_ridge_regression(\n",
    "    sub1['language'],embeddings_t5, n_folds=5, alphas=alphas\n",
    ")\n",
    "\n",
    "# Print results for each alpha\n",
    "print(\"\\nCross-validation results:\")\n",
    "print(\"Alpha\\t2V2 Accuracy\\tPearson Correlation\")\n",
    "print(\"-\" * 50)\n",
    "for alpha in alphas:\n",
    "    print(f\"{alpha}\\t{avg_results[alpha]['2v2_acc']:.4f}\\t\\t{avg_results[alpha]['pearson_corr']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5-fold cross-validation...\n",
      "Fold 1/5\n",
      "Training first-level decoders...\n",
      "  Processing region: language\n",
      "  Processing region: vision\n",
      "  Processing region: task\n",
      "  Processing region: dmn\n",
      "Training meta-regressor...\n",
      "  Completed in 11.33 seconds\n",
      "  2V2 Accuracy: 0.0000\n",
      "  Pearson Correlation: 1.0000\n",
      "  Median Rank: 63.5\n",
      "  Top-1 Accuracy: 0.0079\n",
      "  Top-5 Accuracy: 0.0397\n",
      "Fold 2/5\n",
      "Training first-level decoders...\n",
      "  Processing region: language\n",
      "  Processing region: vision\n",
      "  Processing region: task\n",
      "  Processing region: dmn\n",
      "Training meta-regressor...\n",
      "  Completed in 11.28 seconds\n",
      "  2V2 Accuracy: 0.0000\n",
      "  Pearson Correlation: 1.0000\n",
      "  Median Rank: 63.5\n",
      "  Top-1 Accuracy: 0.0079\n",
      "  Top-5 Accuracy: 0.0397\n",
      "Fold 3/5\n",
      "Training first-level decoders...\n",
      "  Processing region: language\n",
      "  Processing region: vision\n",
      "  Processing region: task\n",
      "  Processing region: dmn\n",
      "Training meta-regressor...\n",
      "  Completed in 11.20 seconds\n",
      "  2V2 Accuracy: 0.0000\n",
      "  Pearson Correlation: 1.0000\n",
      "  Median Rank: 63.0\n",
      "  Top-1 Accuracy: 0.0080\n",
      "  Top-5 Accuracy: 0.0400\n",
      "Fold 4/5\n",
      "Training first-level decoders...\n",
      "  Processing region: language\n",
      "  Processing region: vision\n",
      "  Processing region: task\n",
      "  Processing region: dmn\n",
      "Training meta-regressor...\n",
      "  Completed in 11.37 seconds\n",
      "  2V2 Accuracy: 0.0000\n",
      "  Pearson Correlation: 1.0000\n",
      "  Median Rank: 63.0\n",
      "  Top-1 Accuracy: 0.0080\n",
      "  Top-5 Accuracy: 0.0400\n",
      "Fold 5/5\n",
      "Training first-level decoders...\n",
      "  Processing region: language\n",
      "  Processing region: vision\n",
      "  Processing region: task\n",
      "  Processing region: dmn\n",
      "Training meta-regressor...\n",
      "  Completed in 11.38 seconds\n",
      "  2V2 Accuracy: 0.0000\n",
      "  Pearson Correlation: 1.0000\n",
      "  Median Rank: 63.0\n",
      "  Top-1 Accuracy: 0.0080\n",
      "  Top-5 Accuracy: 0.0400\n",
      "\n",
      "Average metrics across folds:\n",
      "2v2_accuracy: 0.0000\n",
      "pearson_correlation: 1.0000\n",
      "median_rank: 63.2000\n",
      "top_1_accuracy: 0.0080\n",
      "top_5_accuracy: 0.0399\n",
      "top_10_accuracy: 0.0797\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import cosine\n",
    "import time\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class BrainRegionDecoder:\n",
    "    \"\"\"\n",
    "    Decoder class for a single brain region\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.model = Ridge(alpha=alpha)\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def fit(self, voxels, embeddings):\n",
    "        \"\"\"Train the model to predict embeddings from voxels\"\"\"\n",
    "        self.model.fit(voxels, embeddings)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, voxels):\n",
    "        \"\"\"Predict embeddings from voxels\"\"\"\n",
    "        return self.model.predict(voxels)\n",
    "\n",
    "class StackedBrainDecoder:\n",
    "    \"\"\"\n",
    "    Stacked regressor that combines predictions from multiple brain regions\n",
    "    \"\"\"\n",
    "    def __init__(self, alphas=None, meta_alpha=1.0, n_folds=5):\n",
    "        \"\"\"\n",
    "        Initialize the stacked decoder\n",
    "        \n",
    "        Args:\n",
    "            alphas: Dictionary mapping region names to alpha values for Ridge regression\n",
    "            meta_alpha: Alpha value for the meta-regressor\n",
    "            n_folds: Number of folds for cross-validation during level-1 training\n",
    "        \"\"\"\n",
    "        self.region_decoders = {}\n",
    "        self.meta_regressor = Ridge(alpha=meta_alpha)\n",
    "        self.alphas = alphas if alphas is not None else {}\n",
    "        self.n_folds = n_folds\n",
    "        self.meta_alpha = meta_alpha\n",
    "        \n",
    "    def fit(self, region_voxels, embeddings):\n",
    "        \"\"\"\n",
    "        Train the stacked model\n",
    "        \n",
    "        Args:\n",
    "            region_voxels: Dictionary mapping region names to voxel arrays\n",
    "            embeddings: Target embedding array of shape (n_samples, embedding_dim)\n",
    "        \"\"\"\n",
    "        print(\"Training first-level decoders...\")\n",
    "        n_samples = embeddings.shape[0]\n",
    "        \n",
    "        # Generate out-of-fold predictions for each region\n",
    "        kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)\n",
    "        \n",
    "        # Create empty arrays to hold level-1 training data\n",
    "        region_predictions = {}\n",
    "        for region_name in region_voxels:\n",
    "            region_predictions[region_name] = np.zeros_like(embeddings)\n",
    "            \n",
    "        # Generate out-of-fold predictions\n",
    "        for region_name, voxels in region_voxels.items():\n",
    "            print(f\"  Processing region: {region_name}\")\n",
    "            alpha = self.alphas.get(region_name, 1.0)\n",
    "            \n",
    "            for train_idx, val_idx in kf.split(voxels):\n",
    "                # Create and train the decoder for this fold\n",
    "                decoder = BrainRegionDecoder(alpha=alpha)\n",
    "                decoder.fit(voxels[train_idx], embeddings[train_idx])\n",
    "                \n",
    "                # Make predictions for validation fold\n",
    "                region_predictions[region_name][val_idx] = decoder.predict(voxels[val_idx])\n",
    "                \n",
    "            # Train the final decoder on all data\n",
    "            self.region_decoders[region_name] = BrainRegionDecoder(alpha=alpha).fit(voxels, embeddings)\n",
    "        \n",
    "        # Combine predictions from all regions for meta-training\n",
    "        print(\"Training meta-regressor...\")\n",
    "        meta_features = np.hstack([region_predictions[region] for region in region_voxels])\n",
    "        self.meta_regressor.fit(meta_features, embeddings)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, region_voxels):\n",
    "        \"\"\"\n",
    "        Make predictions using the stacked model\n",
    "        \n",
    "        Args:\n",
    "            region_voxels: Dictionary mapping region names to voxel arrays for test samples\n",
    "            \n",
    "        Returns:\n",
    "            Predicted embeddings\n",
    "        \"\"\"\n",
    "        # Make predictions from each region decoder\n",
    "        region_predictions = []\n",
    "        for region_name, voxels in region_voxels.items():\n",
    "            if region_name in self.region_decoders:\n",
    "                region_pred = self.region_decoders[region_name].predict(voxels)\n",
    "                region_predictions.append(region_pred)\n",
    "        \n",
    "        # Combine predictions for meta-regressor\n",
    "        meta_features = np.hstack(region_predictions)\n",
    "        \n",
    "        # Make final predictions\n",
    "        return self.meta_regressor.predict(meta_features)\n",
    "\n",
    "def compute_2v2_accuracy(Y_true, Y_pred):\n",
    "    \"\"\"\n",
    "    Compute 2v2 accuracy between true and predicted embeddings\n",
    "    \"\"\"\n",
    "    N = Y_true.shape[0]\n",
    "    count_correct = 0\n",
    "    total_pairs = 0\n",
    "    \n",
    "    for i in range(N-1):\n",
    "        for j in range(i+1, N):\n",
    "            # Compute cosine distances\n",
    "            cos_dist_i_i_hat = cosine(Y_true[i], Y_pred[i])\n",
    "            cos_dist_j_j_hat = cosine(Y_true[j], Y_pred[j])\n",
    "            cos_dist_i_j_hat = cosine(Y_true[i], Y_pred[j])\n",
    "            cos_dist_j_i_hat = cosine(Y_true[j], Y_pred[i])\n",
    "            \n",
    "            # 2v2 condition: sum of correct pairings < sum of incorrect pairings\n",
    "            if (cos_dist_i_i_hat + cos_dist_j_j_hat) < (cos_dist_i_j_hat + cos_dist_j_i_hat):\n",
    "                count_correct += 1\n",
    "            \n",
    "            total_pairs += 1\n",
    "    \n",
    "    return count_correct / total_pairs\n",
    "\n",
    "def compute_pearson_correlation(Y_true, Y_pred):\n",
    "    \"\"\"\n",
    "    Compute average Pearson correlation between true and predicted embeddings\n",
    "    \"\"\"\n",
    "    N = Y_true.shape[0]\n",
    "    correlations = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        corr, _ = pearsonr(Y_true[i], Y_pred[i])\n",
    "        correlations.append(corr)\n",
    "    \n",
    "    return np.mean(correlations)\n",
    "\n",
    "def compute_rank_accuracy(Y_true, Y_pred):\n",
    "    \"\"\"\n",
    "    Compute median rank of true sentences based on cosine similarity\n",
    "    \n",
    "    For each sample i, we compute the cosine similarity between predicted embedding Y_pred[i]\n",
    "    and all true embeddings Y_true. We then rank the true sentence among all candidates.\n",
    "    \"\"\"\n",
    "    N = Y_true.shape[0]\n",
    "    ranks = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        # Compute cosine similarity between prediction i and all true embeddings\n",
    "        similarities = [1 - cosine(Y_pred[i], Y_true[j]) for j in range(N)]\n",
    "        \n",
    "        # Sort similarities (descending) and get rank of true sentence\n",
    "        sorted_indices = np.argsort(similarities)[::-1]\n",
    "        rank = np.where(sorted_indices == i)[0][0] + 1  # +1 for 1-indexed ranking\n",
    "        ranks.append(rank)\n",
    "    \n",
    "    return np.median(ranks)\n",
    "\n",
    "def compute_top_k_accuracy(Y_true, Y_pred, k=5):\n",
    "    \"\"\"\n",
    "    Compute percentage of cases where true sentence is in top-k predictions\n",
    "    \"\"\"\n",
    "    N = Y_true.shape[0]\n",
    "    top_k_correct = 0\n",
    "    \n",
    "    for i in range(N):\n",
    "        # Compute cosine similarity between prediction i and all true embeddings\n",
    "        similarities = [1 - cosine(Y_pred[i], Y_true[j]) for j in range(N)]\n",
    "        \n",
    "        # Sort similarities (descending) and get rank of true sentence\n",
    "        sorted_indices = np.argsort(similarities)[::-1]\n",
    "        rank = np.where(sorted_indices == i)[0][0] + 1  # +1 for 1-indexed ranking\n",
    "        \n",
    "        if rank <= k:\n",
    "            top_k_correct += 1\n",
    "    \n",
    "    return top_k_correct / N\n",
    "\n",
    "def evaluate_decoder(Y_true, Y_pred, ks=[1, 5, 10]):\n",
    "    \"\"\"\n",
    "    Evaluate decoder performance with multiple metrics\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        '2v2_accuracy': compute_2v2_accuracy(Y_true, Y_pred),\n",
    "        'pearson_correlation': compute_pearson_correlation(Y_true, Y_pred),\n",
    "        'median_rank': compute_rank_accuracy(Y_true, Y_pred)\n",
    "    }\n",
    "    \n",
    "    # Compute top-k accuracy for different k values\n",
    "    for k in ks:\n",
    "        results[f'top_{k}_accuracy'] = compute_top_k_accuracy(Y_true, Y_pred, k=k)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def cross_validate_stacked_decoder(region_voxels, embeddings, n_folds=5, \n",
    "                                  region_alphas=None, meta_alpha=1.0):\n",
    "    \"\"\"\n",
    "    Perform cross-validation to evaluate the stacked decoder\n",
    "    \n",
    "    Args:\n",
    "        region_voxels: Dictionary mapping region names to voxel arrays\n",
    "        embeddings: Target embedding array\n",
    "        n_folds: Number of folds for cross-validation\n",
    "        region_alphas: Dictionary mapping region names to alpha values\n",
    "        meta_alpha: Alpha value for meta-regressor\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Store metrics for each fold\n",
    "    fold_metrics = []\n",
    "    \n",
    "    print(f\"Starting {n_folds}-fold cross-validation...\")\n",
    "    fold = 1\n",
    "    \n",
    "    for train_idx, test_idx in kf.split(embeddings):\n",
    "        print(f\"Fold {fold}/{n_folds}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Prepare training and test data for each region\n",
    "        train_regions = {}\n",
    "        test_regions = {}\n",
    "        \n",
    "        for region_name, voxels in region_voxels.items():\n",
    "            train_regions[region_name] = voxels[train_idx]\n",
    "            test_regions[region_name] = voxels[test_idx]\n",
    "        \n",
    "        # Train and evaluate stacked decoder\n",
    "        stacked_decoder = StackedBrainDecoder(\n",
    "            alphas=region_alphas,\n",
    "            meta_alpha=meta_alpha,\n",
    "            n_folds=3  # Use fewer folds for the internal CV to save time\n",
    "        )\n",
    "        \n",
    "        stacked_decoder.fit(train_regions, embeddings[train_idx])\n",
    "        predictions = stacked_decoder.predict(test_regions)\n",
    "        \n",
    "        # Evaluate performance\n",
    "        metrics = evaluate_decoder(embeddings[test_idx], predictions)\n",
    "        fold_metrics.append(metrics)\n",
    "        \n",
    "        print(f\"  Completed in {time.time() - start_time:.2f} seconds\")\n",
    "        print(f\"  2V2 Accuracy: {metrics['2v2_accuracy']:.4f}\")\n",
    "        print(f\"  Pearson Correlation: {metrics['pearson_correlation']:.4f}\")\n",
    "        print(f\"  Median Rank: {metrics['median_rank']:.1f}\")\n",
    "        print(f\"  Top-1 Accuracy: {metrics['top_1_accuracy']:.4f}\")\n",
    "        print(f\"  Top-5 Accuracy: {metrics['top_5_accuracy']:.4f}\")\n",
    "        \n",
    "        fold += 1\n",
    "    \n",
    "    # Average metrics across folds\n",
    "    avg_metrics = {}\n",
    "    for key in fold_metrics[0].keys():\n",
    "        avg_metrics[key] = np.mean([fold[key] for fold in fold_metrics])\n",
    "    \n",
    "    return avg_metrics, fold_metrics\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example data - replace with actual data loading\n",
    "    n_samples = 627\n",
    "    embedding_dim = 384\n",
    "    \n",
    "    # Create sample region voxels dictionary\n",
    "    # In real usage, load your actual voxel data for each region\n",
    "    region_voxels = {\n",
    "        'language': sub1['language'],\n",
    "        'vision': sub1['vision']    ,\n",
    "        'task': sub1['task'],\n",
    "        'dmn': sub1['dmn']\n",
    "    }\n",
    "\n",
    "    # Define region-specific alphas (optimized separately for each region)\n",
    "    region_alphas = {\n",
    "        'language': 0.01,\n",
    "        'vision': 0.01,\n",
    "        'task': 0.01,\n",
    "        'dmn': 0.01\n",
    "    }\n",
    "    \n",
    "    # Cross-validate stacked decoder\n",
    "    avg_metrics, fold_metrics = cross_validate_stacked_decoder(\n",
    "        region_voxels, \n",
    "        embeddings_clip,\n",
    "        n_folds=5,\n",
    "        region_alphas=region_alphas,\n",
    "        meta_alpha=1.0\n",
    "    )\n",
    "    \n",
    "    # Print average metrics\n",
    "    print(\"\\nAverage metrics across folds:\")\n",
    "    for key, value in avg_metrics.items():\n",
    "        print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5-fold cross-validation...\n",
      "Fold 1/5\n",
      "Training first-level decoders...\n",
      "  Processing region: language\n",
      "  Processing region: vision\n",
      "  Processing region: task\n",
      "  Processing region: dmn\n",
      "Training meta-regressor...\n",
      "  Completed in 12.40 seconds\n",
      "  2V2 Accuracy: 0.9967\n",
      "  Pearson Correlation: 0.3941\n",
      "  Median Rank: 2.0\n",
      "  Top-1 Accuracy: 0.4603\n",
      "  Top-5 Accuracy: 0.7381\n",
      "Fold 2/5\n",
      "Training first-level decoders...\n",
      "  Processing region: language\n",
      "  Processing region: vision\n",
      "  Processing region: task\n",
      "  Processing region: dmn\n",
      "Training meta-regressor...\n",
      "  Completed in 12.45 seconds\n",
      "  2V2 Accuracy: 0.9940\n",
      "  Pearson Correlation: 0.3799\n",
      "  Median Rank: 2.0\n",
      "  Top-1 Accuracy: 0.4444\n",
      "  Top-5 Accuracy: 0.7619\n",
      "Fold 3/5\n",
      "Training first-level decoders...\n",
      "  Processing region: language\n",
      "  Processing region: vision\n",
      "  Processing region: task\n",
      "  Processing region: dmn\n",
      "Training meta-regressor...\n",
      "  Completed in 12.58 seconds\n",
      "  2V2 Accuracy: 0.9939\n",
      "  Pearson Correlation: 0.3714\n",
      "  Median Rank: 2.0\n",
      "  Top-1 Accuracy: 0.4640\n",
      "  Top-5 Accuracy: 0.7600\n",
      "Fold 4/5\n",
      "Training first-level decoders...\n",
      "  Processing region: language\n",
      "  Processing region: vision\n",
      "  Processing region: task\n",
      "  Processing region: dmn\n",
      "Training meta-regressor...\n",
      "  Completed in 12.43 seconds\n",
      "  2V2 Accuracy: 0.9930\n",
      "  Pearson Correlation: 0.3654\n",
      "  Median Rank: 2.0\n",
      "  Top-1 Accuracy: 0.4240\n",
      "  Top-5 Accuracy: 0.7040\n",
      "Fold 5/5\n",
      "Training first-level decoders...\n",
      "  Processing region: language\n",
      "  Processing region: vision\n",
      "  Processing region: task\n",
      "  Processing region: dmn\n",
      "Training meta-regressor...\n",
      "  Completed in 12.71 seconds\n",
      "  2V2 Accuracy: 0.9947\n",
      "  Pearson Correlation: 0.3746\n",
      "  Median Rank: 2.0\n",
      "  Top-1 Accuracy: 0.4240\n",
      "  Top-5 Accuracy: 0.7600\n",
      "\n",
      "Average metrics across folds:\n",
      "2v2_accuracy: 0.9945\n",
      "pearson_correlation: 0.3771\n",
      "median_rank: 2.0000\n",
      "top_1_accuracy: 0.4434\n",
      "top_5_accuracy: 0.7448\n",
      "top_10_accuracy: 0.8581\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_samples = 627\n",
    "embedding_dim = 384\n",
    "\n",
    "# Create sample region voxels dictionary\n",
    "# In real usage, load your actual voxel data for each region\n",
    "region_voxels = {\n",
    "    'language': sub1['language'],\n",
    "    'vision': sub1['vision']    ,\n",
    "    'task': sub1['task'],\n",
    "    'dmn': sub1['dmn']\n",
    "}\n",
    "\n",
    "# Define region-specific alphas (optimized separately for each region)\n",
    "region_alphas = {\n",
    "    'language': 0.01,\n",
    "    'vision': 0.01,\n",
    "    'task': 0.01,\n",
    "    'dmn': 0.01\n",
    "}\n",
    "\n",
    "# Cross-validate stacked decoder\n",
    "avg_metrics, fold_metrics = cross_validate_stacked_decoder(\n",
    "    region_voxels, \n",
    "    embeddings_large,\n",
    "    n_folds=5,\n",
    "    region_alphas=region_alphas,\n",
    "    meta_alpha=1.0\n",
    ")\n",
    "\n",
    "# Print average metrics\n",
    "print(\"\\nAverage metrics across folds:\")\n",
    "for key, value in avg_metrics.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5-fold cross-validation...\n",
      "Fold 1/5\n",
      "Training first-level decoders...\n",
      "  Processing region: language\n",
      "  Processing region: vision\n",
      "  Processing region: task\n",
      "  Processing region: dmn\n",
      "Training meta-regressor...\n",
      "  Completed in 10.35 seconds\n",
      "  2V2 Accuracy: 0.9940\n",
      "  Pearson Correlation: 0.3622\n",
      "  Median Rank: 2.0\n",
      "  Top-1 Accuracy: 0.4286\n",
      "  Top-5 Accuracy: 0.7143\n",
      "Fold 2/5\n",
      "Training first-level decoders...\n",
      "  Processing region: language\n",
      "  Processing region: vision\n",
      "  Processing region: task\n",
      "  Processing region: dmn\n",
      "Training meta-regressor...\n",
      "  Completed in 10.68 seconds\n",
      "  2V2 Accuracy: 0.9956\n",
      "  Pearson Correlation: 0.3568\n",
      "  Median Rank: 2.0\n",
      "  Top-1 Accuracy: 0.4206\n",
      "  Top-5 Accuracy: 0.7857\n",
      "Fold 3/5\n",
      "Training first-level decoders...\n",
      "  Processing region: language\n",
      "  Processing region: vision\n",
      "  Processing region: task\n",
      "  Processing region: dmn\n",
      "Training meta-regressor...\n",
      "  Completed in 10.62 seconds\n",
      "  2V2 Accuracy: 0.9945\n",
      "  Pearson Correlation: 0.3418\n",
      "  Median Rank: 2.0\n",
      "  Top-1 Accuracy: 0.4080\n",
      "  Top-5 Accuracy: 0.7200\n",
      "Fold 4/5\n",
      "Training first-level decoders...\n",
      "  Processing region: language\n",
      "  Processing region: vision\n",
      "  Processing region: task\n",
      "  Processing region: dmn\n",
      "Training meta-regressor...\n",
      "  Completed in 10.48 seconds\n",
      "  2V2 Accuracy: 0.9914\n",
      "  Pearson Correlation: 0.3351\n",
      "  Median Rank: 3.0\n",
      "  Top-1 Accuracy: 0.4240\n",
      "  Top-5 Accuracy: 0.6960\n",
      "Fold 5/5\n",
      "Training first-level decoders...\n",
      "  Processing region: language\n",
      "  Processing region: vision\n",
      "  Processing region: task\n",
      "  Processing region: dmn\n",
      "Training meta-regressor...\n",
      "  Completed in 10.47 seconds\n",
      "  2V2 Accuracy: 0.9912\n",
      "  Pearson Correlation: 0.3451\n",
      "  Median Rank: 2.0\n",
      "  Top-1 Accuracy: 0.4080\n",
      "  Top-5 Accuracy: 0.7040\n",
      "\n",
      "Average metrics across folds:\n",
      "2v2_accuracy: 0.9933\n",
      "pearson_correlation: 0.3482\n",
      "median_rank: 2.2000\n",
      "top_1_accuracy: 0.4178\n",
      "top_5_accuracy: 0.7240\n",
      "top_10_accuracy: 0.8420\n"
     ]
    }
   ],
   "source": [
    "n_samples = 627\n",
    "embedding_dim = 384\n",
    "\n",
    "# Create sample region voxels dictionary\n",
    "# In real usage, load your actual voxel data for each region\n",
    "region_voxels = {\n",
    "    'language': sub1['language'],\n",
    "    'vision': sub1['vision']    ,\n",
    "    'task': sub1['task'],\n",
    "    'dmn': sub1['dmn']\n",
    "}\n",
    "\n",
    "\n",
    "# Define region-specific alphas (optimized separately for each region)\n",
    "region_alphas = {\n",
    "    'language': 0.01,\n",
    "    'vision': 0.01,\n",
    "    'task': 0.01,\n",
    "    'dmn': 0.01\n",
    "}\n",
    "\n",
    "# Cross-validate stacked decoder\n",
    "avg_metrics, fold_metrics = cross_validate_stacked_decoder(\n",
    "    region_voxels, \n",
    "    embeddings_small,\n",
    "    n_folds=5,\n",
    "    region_alphas=region_alphas,\n",
    "    meta_alpha=1.0\n",
    ")\n",
    "\n",
    "# Print average metrics\n",
    "print(\"\\nAverage metrics across folds:\")\n",
    "for key, value in avg_metrics.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5-fold cross-validation...\n",
      "Fold 1/5\n",
      "Training first-level decoders...\n",
      "  Processing region: language\n",
      "  Processing region: vision\n",
      "  Processing region: task\n",
      "  Processing region: dmn\n",
      "Training meta-regressor...\n",
      "  Completed in 12.48 seconds\n",
      "  2V2 Accuracy: 0.9919\n",
      "  Pearson Correlation: 0.6514\n",
      "  Median Rank: 3.0\n",
      "  Top-1 Accuracy: 0.3175\n",
      "  Top-5 Accuracy: 0.6905\n",
      "Fold 2/5\n",
      "Training first-level decoders...\n",
      "  Processing region: language\n",
      "  Processing region: vision\n",
      "  Processing region: task\n",
      "  Processing region: dmn\n",
      "Training meta-regressor...\n",
      "  Completed in 12.70 seconds\n",
      "  2V2 Accuracy: 0.9897\n",
      "  Pearson Correlation: 0.6485\n",
      "  Median Rank: 3.0\n",
      "  Top-1 Accuracy: 0.3333\n",
      "  Top-5 Accuracy: 0.6349\n",
      "Fold 3/5\n",
      "Training first-level decoders...\n",
      "  Processing region: language\n",
      "  Processing region: vision\n",
      "  Processing region: task\n",
      "  Processing region: dmn\n",
      "Training meta-regressor...\n",
      "  Completed in 12.85 seconds\n",
      "  2V2 Accuracy: 0.9897\n",
      "  Pearson Correlation: 0.6461\n",
      "  Median Rank: 4.0\n",
      "  Top-1 Accuracy: 0.3200\n",
      "  Top-5 Accuracy: 0.5920\n",
      "Fold 4/5\n",
      "Training first-level decoders...\n",
      "  Processing region: language\n",
      "  Processing region: vision\n",
      "  Processing region: task\n",
      "  Processing region: dmn\n",
      "Training meta-regressor...\n",
      "  Completed in 12.59 seconds\n",
      "  2V2 Accuracy: 0.9881\n",
      "  Pearson Correlation: 0.6380\n",
      "  Median Rank: 3.0\n",
      "  Top-1 Accuracy: 0.3360\n",
      "  Top-5 Accuracy: 0.6560\n",
      "Fold 5/5\n",
      "Training first-level decoders...\n",
      "  Processing region: language\n",
      "  Processing region: vision\n",
      "  Processing region: task\n",
      "  Processing region: dmn\n",
      "Training meta-regressor...\n",
      "  Completed in 12.87 seconds\n",
      "  2V2 Accuracy: 0.9948\n",
      "  Pearson Correlation: 0.6444\n",
      "  Median Rank: 3.0\n",
      "  Top-1 Accuracy: 0.3120\n",
      "  Top-5 Accuracy: 0.6160\n",
      "\n",
      "Average metrics across folds:\n",
      "2v2_accuracy: 0.9908\n",
      "pearson_correlation: 0.6457\n",
      "median_rank: 3.2000\n",
      "top_1_accuracy: 0.3238\n",
      "top_5_accuracy: 0.6379\n",
      "top_10_accuracy: 0.7784\n"
     ]
    }
   ],
   "source": [
    "n_samples = 627\n",
    "embedding_dim = 384\n",
    "\n",
    "# Create sample region voxels dictionary\n",
    "# In real usage, load your actual voxel data for each region\n",
    "region_voxels = {\n",
    "    'language': sub1['language'],\n",
    "    'vision': sub1['vision']    ,\n",
    "    'task': sub1['task'],\n",
    "    'dmn': sub1['dmn']\n",
    "}\n",
    "\n",
    "\n",
    "# Define region-specific alphas (optimized separately for each region)\n",
    "region_alphas = {\n",
    "    'language': 0.01,\n",
    "    'vision': 0.01,\n",
    "    'task': 0.01,\n",
    "    'dmn': 0.01\n",
    "}\n",
    "\n",
    "# Cross-validate stacked decoder\n",
    "avg_metrics, fold_metrics = cross_validate_stacked_decoder(\n",
    "    region_voxels, \n",
    "    embeddings_t5,\n",
    "    n_folds=5,\n",
    "    region_alphas=region_alphas,\n",
    "    meta_alpha=1.0\n",
    ")\n",
    "\n",
    "# Print average metrics\n",
    "print(\"\\nAverage metrics across folds:\")\n",
    "for key, value in avg_metrics.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "l2s",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
